<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Liveness Detection with Blur</title>
    <style>
        body,
        html {
            margin: 0;
            padding: 0;
            width: 100%;
            height: 100%;
            background: black;
            display: flex;
            justify-content: center;
            align-items: center;
            flex-direction: column;
        }

        /* Container for video and overlay */
        #container {
            position: relative;
            width: 640px;
            height: 470px;
        }

        video {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        /* Oval overlay for face alignment */
        #overlay {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            border: 4px solid red;
            /* Default Red (Error) */
            width: 250px;
            height: 350px;
            border-radius: 50%;
            pointer-events: none;
        }

        #status {
            margin-top: 10px;
            color: white;
            font-size: 20px;
        }

        /* Blur effect outside oval */
        #blurOverlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            backdrop-filter: blur(40px);
            -webkit-backdrop-filter: blur(40px);
            /* clip-path: path('M0,0H640V480H0Z M235,110 A125,175,0,1,0,319.9,65Z'); */
            clip-path: path('M0,0H640V480H0Z M319,65 A125,175,0,1,0,319.9,65Z');
            pointer-events: none;
        }

    </style>
</head>

<body>
    <div id="container">
        <video id="video" autoplay playsinline></video>
        <div id="blurOverlay"></div>
        <div id="overlay"></div>
    </div>
    <div id="status">Loading models...</div>

    <!-- Load face-api.js from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1/dist/face-api.js"></script>

    <script>

        const video = document.getElementById("video");
        const statusText = document.getElementById("status");
        const overlay = document.getElementById("overlay");

        let blinkState = "OPEN"; // To track blink status
        let faceInsideOval = false;

        async function setupCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
                video.srcObject = stream;
            } catch (error) {
                statusText.textContent = "Webcam access denied!";
                console.error(error);
            }
        }

        async function loadModels() {
            await faceapi.nets.tinyFaceDetector.loadFromUri("models");
            await faceapi.nets.faceLandmark68Net.loadFromUri("models");
            await faceapi.nets.faceRecognitionNet.loadFromUri('/models'),
            await faceapi.nets.faceExpressionNet.loadFromUri('/models')
            statusText.textContent = "Models loaded. Detecting face...";
            detectFace();
        }

        function isPointInOval(point, oval) {
            let cx = oval.left + oval.width / 2;
            let cy = oval.top + oval.height / 2;
            let rx = oval.width / 2;
            let ry = oval.height / 2;
            let dx = point.x - cx;
            let dy = point.y - cy;
            // Adjust face center based on this threshold ex: 0.1            
            return (dx * dx) / (rx * rx) + (dy * dy) / (ry * ry) <= 0.1;
        }

        async function detectFace() {
            const videoRect = video.getBoundingClientRect();
            const ovalRect = overlay.getBoundingClientRect();

            setInterval(async () => {
                const detections = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks();

                if (detections) {
                    let faceBox = detections.detection.box;                    
                    let faceCenter = { x: faceBox.x + faceBox.width / 2, y: faceBox.y + faceBox.height / 2 };

                    let scaleX = videoRect.width / video.videoWidth;
                    let scaleY = videoRect.height / video.videoHeight;

                    let faceCenterViewport = {
                        x: videoRect.left + faceCenter.x * scaleX,
                        y: videoRect.top + faceCenter.y * scaleY
                    };

                    faceInsideOval = isPointInOval(faceCenterViewport, ovalRect);

                    if (faceInsideOval) {
                        detectBlink(detections);
                        overlay.style.borderColor = "green"; // Green if inside oval
                        statusText.textContent = "Face detected inside oval. Blink to confirm!";
                    } else {
                        overlay.style.borderColor = "red"; // Red if outside
                        statusText.textContent = "Align face inside the oval!";
                    }

                } else {
                    overlay.style.borderColor = "red";
                    statusText.textContent = "No face detected!";
                }
            }, 500);
        }

        function detectBlink(detection) {
            let landmarks = detection.landmarks;
            let leftEye = landmarks.getLeftEye();
            let rightEye = landmarks.getRightEye();

            let leftEyeOpen = eyeOpenRatio(leftEye);
            let rightEyeOpen = eyeOpenRatio(rightEye);

            if (blinkState === "OPEN" && leftEyeOpen < 0.2 && rightEyeOpen < 0.2) {
                blinkState = "CLOSED";
                statusText.textContent = "Now open your eyes!";
            }

            if (blinkState === "CLOSED" && leftEyeOpen > 0.5 && rightEyeOpen > 0.5) {
                blinkState = "COMPLETE";
                if (faceInsideOval) {
                    statusText.textContent = "Liveness confirmed!";
                    overlay.style.borderColor = "green";
                }
            }
        }

        function eyeOpenRatio(eye) {
            let verticalDist = Math.abs(eye[1].y - eye[5].y);
            let horizontalDist = Math.abs(eye[0].x - eye[3].x);
            return verticalDist / horizontalDist;
        }

        // Initialize Webcam and Load Models
        setupCamera().then(loadModels);
    </script>
</body>

</html>